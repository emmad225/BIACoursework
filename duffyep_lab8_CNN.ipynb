{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmad225/BIACoursework/blob/main/duffyep_lab8_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ag0r1CeJ-7X"
      },
      "source": [
        "# CSCI 3397 Lab 8: Image Prediction with CNN Models\n",
        "\n",
        "**Posted:** Saturday, March 30, 2024\n",
        "\n",
        "**Due:** Monday, April 8, 2024\n",
        "\n",
        "__Total Points__: 11 pts\n",
        "\n",
        "__Submission__: please rename the .ipynb file as __\\<your_username\\>\\_lab8.ipynb__ before you submit it to canvas. Example: weidf_lab8.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>1. CNN models [11 pts] </b>\n",
        "\n",
        "After learning AlexNet, you need to be comfortale to implement other variants based on descriptions. Let's try out for one old CNN and one new CNN."
      ],
      "metadata": {
        "id": "c_NV7paYRuUS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPvy-9UPKU9l"
      },
      "source": [
        "## 1.1 LeNet [5 pts]\n",
        "\n",
        "The Yann LeCun et al. (1989) paper <a href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf\">Backpropagation Applied to Handwritten Zip Code Recognition</a> is widely considered as the earliest real-world application of a convolutional neural net. Except for the tiny dataset and model, this paper still reads modern even after 33 years with descriptions on the dataset, model architecture, loss function, optimization, and classification error rates ovr training and test sets. Today, let's reproduce it with PyTorch!\n",
        "\n",
        "<img src=\"https://karpathy.github.io/assets/lecun/lecun1989.png\"/>\n",
        "\n",
        "\n",
        "In this pset, you will be reading different tutorials to implement the concepts learned in class. Happy Hacking!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model described in the paper is a bit non-standard by now. We will implement a simplified version instead:\n",
        "\n",
        "```\n",
        "- Input: 16x16x3 (in_channels)\n",
        "- Feature Extraction\n",
        "  * Layer H1 (conv): kernel_size=5x5, #kernel=12, stride=2, padding=2\n",
        "  * Nonlinearity: nn.tanh\n",
        "  * Layer H2 (conv): kernel size=5x5, #kernel=12, stride=2, padding=2\n",
        "  * Nonlinearity: nn.tanh\n",
        "- Reshape Layer: torch.flatten\n",
        "- Classifier\n",
        "  * Layer H3 (linear): #hidden units=30\n",
        "  * Nonlinearity: nn.tanh\n",
        "  * Output (linear): #hidden units=9 #Donglai said to change this to 9\n",
        "```\n",
        "\n",
        "**TODO:**\n",
        "- Read and understand the example AlexNet code [link](https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py) (line 18-53)\n",
        "- Follow the example and implement the class `CNN1989` based on the description above.\n",
        "\n",
        "Hints:\n",
        "- For each type of layer, you can check out the documentation: [conv2d layer](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html), [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
        "- The loss function below will take care of the softmax normalization of the prediction"
      ],
      "metadata": {
        "id": "vvs_qpQCO2tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN1989(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    #### your code starts ####\n",
        "    self.features = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=12, kernel_size=5, stride=2, padding=2), nn.Tanh(), nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=2, padding=2), nn.Tanh())\n",
        "    self.classifier = nn.Sequential(nn.Linear(12*4*4, 30), nn.Tanh(), nn.Linear(30, 9))\n",
        "    #### your code ends ####\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    #### your code starts ####\n",
        "    x = self.features(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.classifier(x)\n",
        "    #### your code ends ####\n",
        "    return x"
      ],
      "metadata": {
        "id": "neis1BNxS2Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 ResNet [5 pts]\n",
        "\n"
      ],
      "metadata": {
        "id": "8H6k5wGbT1bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Basic block [3 pts]\n",
        "\n",
        "<img height=150 src=\"https://neurohive.io/wp-content/uploads/2019/01/resnet-e1548261477164.png\" />"
      ],
      "metadata": {
        "id": "rNST5da8jcx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            # make sure the shortcut has the same dimension\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #### Your code starts here ####\n",
        "        # finish the forward pass\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Shortcut connection\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        #### Your code ends here ####\n",
        "        return out"
      ],
      "metadata": {
        "id": "lsmv6_hpjc7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) ResNet-18 model [3 pts]\n",
        "Let's implement the smallest version of ResNet.\n",
        "\n",
        "<img height=400 src=\"https://pytorch.org/assets/images/resnet.png\">"
      ],
      "metadata": {
        "id": "4MqTRdaYjpGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # conv1: conv -> bn -> relu\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # other conv layers\n",
        "        #### your code starts ####\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        #### your code ends ####\n",
        "\n",
        "        # avg pooling\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        # reshape\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # classification\n",
        "        #### your code starts ####\n",
        "        out = self.linear(out)\n",
        "        #### your code ends ####\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "metadata": {
        "id": "jxLaIEZZkEUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYYOGaO2F8o3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}