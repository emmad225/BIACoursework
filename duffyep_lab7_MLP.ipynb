{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmad225/BIACoursework/blob/main/duffyep_lab7_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAae8nayHhH2"
      },
      "source": [
        "# [CSCI 3397/PSYC 3317] Lab 7: MLP\n",
        "\n",
        "**Posted:** Wednesday, March 20, 2024\n",
        "\n",
        "**Due:** Wednesday, March 27, 2024\n",
        "\n",
        "__Total Points__: 8 pts\n",
        "\n",
        "__Submission__: please rename the .ipynb file as __\\<your_username\\>\\_lab7.ipynb__ before you submit it to canvas. Example: weidf_lab7.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>1. Model</b>\n",
        "\n"
      ],
      "metadata": {
        "id": "WZTLi12JU4Ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two-layer MLP (1 hidden layer)\n",
        "- Pytorch Basics: To build a deep learning model in Pytorch, we need to define the needed layers under `__init__()` and specify the model computation under `foward()`. The gradient computation is automatically done under the parent's `backward()` (can be overwritten if needed).\n",
        "- Example: a 2-layer MLP model with 10-dim input, 5-dim output, and 20 neurons for the hidden layer."
      ],
      "metadata": {
        "id": "BOOFO09DYW0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLP_oneHiddenLayer(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, num_neuron, nonlinear=F.relu):\n",
        "    super(MLP_oneHiddenLayer, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(input_dim, num_neuron)\n",
        "    self.fc2 = nn.Linear(num_neuron, output_dim)\n",
        "    self.nonlinear = nonlinear\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "Ad-BY636U4S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_input, num_output, num_neuron = 10, 5, 20\n",
        "model = MLP_oneHiddenLayer(num_input, num_output, num_neuron)\n",
        "\n",
        "batch_size = 32\n",
        "result = model(torch.zeros([batch_size, num_input]))\n",
        "print('input size:', [batch_size, num_input])\n",
        "print('output size:', result.shape)"
      ],
      "metadata": {
        "id": "Pu8OFHB0VB-G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3aeb021-56d5-4458-e9a2-938bbd72ff53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size: [32, 10]\n",
            "output size: torch.Size([32, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3 pts] Exercise 1: N-layer MLP\n",
        "- Let's build a MLP model with the input number of hidden layers and number of neurons.\n",
        "- In Pytorch, we can first create a list of layers `layers=[..]` and then use `nn.Sequential(*layers)` to chain them up in the computation, which is equivalent to using the for-loop."
      ],
      "metadata": {
        "id": "La2xLRiDVD-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, num_neuron=[], nonlinear=F.relu):\n",
        "    super(MLP, self).__init__()\n",
        "    layers = []\n",
        "    if len(num_neuron) == 0:\n",
        "      layers += [nn.Linear(input_dim, output_dim)]\n",
        "    else:\n",
        "      # Manually write out the first layer\n",
        "      layers.append(nn.Linear(input_dim, num_neuron[0]))\n",
        "      # Loop through the middle layers\n",
        "      for i in range(len(num_neuron) - 1):\n",
        "        layers.append(nn.Linear(num_neuron[i], num_neuron[i + 1]))\n",
        "      # Manually write out the last layer\n",
        "      layers.append(nn.Linear(num_neuron[-1], output_dim))\n",
        "\n",
        "    self.layers = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch    x = x.view(-1, 32*32*3)\n",
        "    x = self.layers(x)\n",
        "    return F.softmax(x, dim=1)\n",
        "\n",
        "# test case\n",
        "num_input, num_output = 10,20\n",
        "num_neuron = [128, 128, 128]\n",
        "model_mlp = MLP(num_input, num_output, num_neuron)\n",
        "\n",
        "batch_size = 10\n",
        "result = model_mlp(torch.zeros([batch_size, num_input]))\n",
        "print('input size:', [batch_size, num_input])\n",
        "print('output size:', result.shape)"
      ],
      "metadata": {
        "id": "xBnEpU97VFH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1bc8c56-382a-41a7-bfda-ec3355464ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input size: [10, 10]\n",
            "output size: torch.Size([10, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>2. Optimization: Backpropagation (BP)</b>\n",
        "\n",
        "- The take-home message is: BP is dynamic programming(DP)\n",
        "\n",
        "- We'll compute the gradient for each variable by hand and compare with Pytorch's autograd result for a 2-layer MLP model. (Lec 23, page 22-26)\n"
      ],
      "metadata": {
        "id": "qykVLmbE3nyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# create a MLP model with all intermediate variables\n",
        "class MLP_oneHiddenLayer_var(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, num_neuron, nonlinear=F.relu):\n",
        "    super(MLP_oneHiddenLayer_var, self).__init__()\n",
        "\n",
        "    self.fc0 = nn.Linear(input_dim, num_neuron)\n",
        "    self.fc1 = nn.Linear(num_neuron, output_dim)\n",
        "    self.nonlinear = nonlinear\n",
        "    self.x1, self.x2, self.x3, self.x4 = None, None, None, None\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    self.x1 = self.fc0(x)\n",
        "    self.x2 = F.relu(self.x1)\n",
        "    self.x3 = self.fc1(self.x2)\n",
        "    self.x4 = F.relu(self.x3)\n",
        "\n",
        "    # by default/to save memory, the gradient for non-leaf nodes\n",
        "    # (intermediate variables in the computation graph) won't be saved\n",
        "    self.x1.retain_grad()\n",
        "    self.x2.retain_grad()\n",
        "    self.x3.retain_grad()\n",
        "    self.x4.retain_grad()\n",
        "\n",
        "    return self.x4\n",
        "\n",
        "model = MLP_oneHiddenLayer_var(input_dim=10, output_dim=20, num_neuron=5)\n",
        "# input size: batch size x input dimension\n",
        "input = torch.rand([1,10])\n",
        "\n",
        "# forward pass\n",
        "output = model(input)\n",
        "target = torch.rand([1,20])\n",
        "# reduction='sum': L2 norm of the difference\n",
        "loss = F.mse_loss(output, target, reduction = 'sum')\n",
        "\n",
        "# backward pass (autograd)\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "HEIJHwz23xio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1 pt] Exercise 2.1: Gradient of the loss layer\n",
        "\n",
        "**Couse material: Lec 23, page 23**"
      ],
      "metadata": {
        "id": "EHXeN7Z832ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient computed by pytorch\n",
        "grad_x4_pt = model.x4.grad\n",
        "grad_x4_manual = 2* (model.x4.grad - target)\n",
        "\n",
        "print('x4: max difference between gt and yours:', (grad_x4_pt - grad_x4_manual.reshape(-1)).abs().max())"
      ],
      "metadata": {
        "id": "Mp_vK81432hK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "384010c9-e939-43e7-8cef-05920f3e5033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x4: max difference between gt and yours: tensor(3.6861)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1 pt] Exercise 2.2: Gradient of the ReLU layer\n",
        "\n",
        "**Couse material: Lec 23, page 24-25**"
      ],
      "metadata": {
        "id": "PrOc-PZg33HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grad_x3_pt = model.x3.grad\n",
        "grad_x3_manual =  grad_x4_manual * (model.x3 > 0).float()\n",
        "\n",
        "print('x3: max difference between gt and yours:', (grad_x3_pt - grad_x3_manual.reshape(-1)).abs().max())"
      ],
      "metadata": {
        "id": "0XDA4-yh33OM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ef3372-769c-4d07-c7e2-cd56b3bb052d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x3: max difference between gt and yours: tensor(2.9955)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3 pts] Exercise 2.3: Gradient of the Linear layer.\n",
        "The $W$ in the slide is the concatenation of $W$ and $b$: `W = [fc.weight, fc.bias.reshape(-1,1)]`\n",
        "\n",
        "\n",
        "**Couse material: Lec 23, page 26**"
      ],
      "metadata": {
        "id": "HsFO1wkz320R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grad_x2_pt = model.x2.grad\n",
        "grad_W1_W_pt = model.fc1.weight.grad\n",
        "grad_W1_b_pt = model.fc1.bias.grad\n",
        "\n",
        "grad_x2_manual = torch.mm(grad_x3_manual, model.fc1.weight)\n",
        "grad_W1_W_manual = torch.mm(model.x2.t(), grad_x3_manual)\n",
        "grad_W1_b_manual = grad_x3_manual.sum(dim=0)\n",
        "\n",
        "print('x2: max difference between gt and yours:', (grad_x2_pt - grad_x2_manual.reshape(-1)).abs().max())\n",
        "print('W1_W: max difference between gt and yours:', (grad_W1_W_pt.reshape(-1) - grad_W1_W_manual.reshape(-1)).abs().max())\n",
        "print('W1_b: max difference between gt and yours:', (grad_W1_b_pt.reshape(-1) - grad_W1_b_manual.reshape(-1)).abs().max())"
      ],
      "metadata": {
        "id": "9ZHcNbQr3267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e23b38-ec00-454d-d7d3-bbeee9aa5393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x2: max difference between gt and yours: tensor(2.4661, grad_fn=<MaxBackward1>)\n",
            "W1_W: max difference between gt and yours: tensor(1.8607, grad_fn=<MaxBackward1>)\n",
            "W1_b: max difference between gt and yours: tensor(2.9955)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c32mY4ClGgb8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}