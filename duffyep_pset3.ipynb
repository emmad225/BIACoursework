{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmad225/BIACoursework/blob/main/duffyep_pset3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ag0r1CeJ-7X"
      },
      "source": [
        "# [CSCI 3397/PSYC 3317] Pset 3: AlexNet\n",
        "\n",
        "**Posted:** Wednesday, March 20, 2024\n",
        "\n",
        "**Due:** Wednesday, April 3, 2024\n",
        "\n",
        "__Total Points__: 19 pts\n",
        "\n",
        "__Submission__: please rename the .ipynb file as __\\<your_username\\>\\_pset3.ipynb__ before you submit it to canvas. Example: weidf_pset3.ipynb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPvy-9UPKU9l"
      },
      "source": [
        "To help you understand AlexNet, let's make one with Numpy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE6Kip8UAEHM"
      },
      "source": [
        "# <b>[6 pts] 1. Layer Input/Output Size</b>\n",
        "\n",
        "Below is the detailed architecture details of the famous AlexNet. Let's go through the details of its layers by computing the input/output size.\n",
        "\n",
        "<img height=400 src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-19-16-01-03.png\"/>\n",
        "<img height=400 src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-19-16-01-13.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caaexTp3AEJ6"
      },
      "source": [
        "## (a) [2 pts] Conv Layer\n",
        "Implement the function to compute the output size of a convolutional layer.\n",
        "\n",
        "**Course Material: Lecture 16, page 8**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By29_3P6AwVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afbada7-eb38-4eb4-9d65-0e3c485adba2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def getSizeConv(input_size, kernel_size, pad_size, stride_size):\n",
        "  # input_size: N x C_in x H x W\n",
        "  # kernel_size: C_out x C_in x KH x KW\n",
        "  # stride: [Sx, Sy]\n",
        "  # pad: [Px, Py]\n",
        "  # -----\n",
        "\n",
        "\n",
        "  ### Your code starts here\n",
        "  # output_size: N x C_out x OH x OW\n",
        "  output_size = np.zeros(4)\n",
        "  # 0: batch_size\n",
        "  output_size[0] = input_size[0]\n",
        "  # 1: channel size\n",
        "  output_size[1] = kernel_size[0]\n",
        "  # 2/3: spatial dimension (height/weight)\n",
        "  output_size[2] = ((input_size[2] + 2 * pad_size[0] - kernel_size[2]) // stride_size[0]) + 1\n",
        "  output_size[3] = ((input_size[3] + 2 * pad_size[1] - kernel_size[3]) // stride_size[1]) + 1\n",
        "  ### Your code ends here\n",
        "\n",
        "  return output_size.astype(int)\n",
        "\n",
        "\n",
        "## test case\n",
        "input_size = [10,3,227,227]\n",
        "kernel_size = [96,3,11,11]\n",
        "pad_size = [0,0]\n",
        "stride_size = [4,4]\n",
        "\n",
        "output_gt = [10,96,55,55]\n",
        "output_pred = getSizeConv(input_size, kernel_size, pad_size, stride_size)\n",
        "\n",
        "print('gt: ', output_gt)\n",
        "print('pred: ', output_pred)\n",
        "print('max abs diff: ', np.abs(output_pred-output_gt).max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt:  [10, 96, 55, 55]\n",
            "pred:  [10 96 55 55]\n",
            "max abs diff:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkPx_BZ2E9_I"
      },
      "source": [
        "## (b) [2 pts] Pooling layer\n",
        "Implement the function to compute the output size of a pooling layer.\n",
        "\n",
        "**Course Material: Lecture 16, page 25**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3aGzXFYE-Is",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7642cff0-ce8e-49af-d545-0b3c58cdc4d2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def getSizePool(input_size, kernel_size, pad_size, stride_size):\n",
        "  # input_size: N x C_in x H x W\n",
        "  # kernel_size: KH x KW\n",
        "  # pad_size: [Px, Py]\n",
        "  # stride_size: [Sx, Sy]\n",
        "  # -----\n",
        "  # output_size: N x C_in x OH x OW\n",
        "\n",
        "  if isinstance(kernel_size, int):\n",
        "    kernel_size = [kernel_size, kernel_size]\n",
        "  if isinstance(stride_size, int):\n",
        "    stride_size = [stride_size, stride_size]\n",
        "  if isinstance(pad_size, int):\n",
        "    pad_size = [pad_size, pad_size]\n",
        "\n",
        "  ### Your code starts here\n",
        "  output_size = np.zeros(4)\n",
        "  # 0: batch_size\n",
        "  output_size[0] = input_size[0]\n",
        "  # 1: channel size\n",
        "  output_size[1] = input_size[1]\n",
        "  # 2/3: spatial dimension (height/weight)\n",
        "  output_size[2] = ((input_size[2] + 2 * pad_size[0] - kernel_size[0]) // stride_size[0]) + 1\n",
        "  output_size[3] = ((input_size[3] + 2 * pad_size[1] - kernel_size[1]) // stride_size[1]) + 1\n",
        "\n",
        "  ### Your code ends here\n",
        "\n",
        "  return output_size.astype(int)\n",
        "\n",
        "\n",
        "## test case\n",
        "input_size = [10,96,55,55]\n",
        "kernel_size = [3,3]\n",
        "stride_size = [2,2]\n",
        "pad_size = [0,0]\n",
        "output_gt = [10,96,27,27]\n",
        "output_pred = getSizePool(input_size, kernel_size, pad_size, stride_size)\n",
        "\n",
        "print('gt: ', output_gt)\n",
        "print('pred: ', output_pred)\n",
        "print('max abs diff: ', np.abs(output_pred-output_gt).max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt:  [10, 96, 27, 27]\n",
            "pred:  [10 96 27 27]\n",
            "max abs diff:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe0tmjrGF3CW"
      },
      "source": [
        "## (c) [1 pt] FC layer\n",
        "Implement the function to compute the output size of a fully-connected (fc) layer.\n",
        "\n",
        "**Course Material: Lecture 14, page 23**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2uHR8Q-F3Ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6399989f-0504-4035-c8d3-cdefd51aa3f8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def getSizeFc(input_size, weight_size):\n",
        "  # input_size: N x L\n",
        "  # weight_size: M x L\n",
        "  # -----\n",
        "  # output_size: N x M\n",
        "\n",
        "  ### Your code starts here\n",
        "  output_size = np.array([input_size[0], weight_size[0]])\n",
        "  ### Your code ends here\n",
        "  return output_size\n",
        "\n",
        "\n",
        "## test case\n",
        "input_size = [10, 4096]\n",
        "weight_size = [1000, 4096]\n",
        "output_gt = [10,1000]\n",
        "output_pred = getSizeFc(input_size, weight_size)\n",
        "\n",
        "print('gt: ', output_gt)\n",
        "print('pred: ', output_pred)\n",
        "print('max abs diff: ', np.abs(output_pred-output_gt).max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt:  [10, 1000]\n",
            "pred:  [  10 1000]\n",
            "max abs diff:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xvUr2aWCMUF"
      },
      "source": [
        "## (d) [1 pt] Reshape Layer\n",
        "Implement the function to compute the output size of a reshape layer. Hint: reshape the feature into 1-dim for each sample in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJu6oBKCCMmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d00f417-5f89-4280-e390-f7292fa93c28"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def getSizeReshape(input_size):\n",
        "  # input_size: N x ... (multi-dim)\n",
        "  # -----\n",
        "  # output_size: N x O\n",
        "  output_size = np.array([input_size[0], np.prod(input_size[1:])])\n",
        "  return output_size\n",
        "\n",
        "\n",
        "## test case\n",
        "input_size = [10, 256, 6, 6]\n",
        "output_gt = [10, 9216]\n",
        "output_pred = getSizeReshape(input_size)\n",
        "\n",
        "print('gt: ', output_gt)\n",
        "print('pred: ', output_pred)\n",
        "print('max abs diff: ', np.abs(output_pred-output_gt).max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gt:  [10, 9216]\n",
            "pred:  [  10 9216]\n",
            "max abs diff:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jf4ooeq-LCS"
      },
      "source": [
        "Here's the output size for each layer of AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grWugoqn-LJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a7a96b-80a9-4773-a07d-0a0919742779"
      },
      "source": [
        "alexNet = [\\\n",
        "\t\t['conv1',[96,3,11,11],[0,0],[4,4]],\\\n",
        "\t\t['pool1',[3,3],[0,0],[2,2]],\\\n",
        "\t\t['conv2',[256,96,5,5],[2,2],[1,1]],\\\n",
        "\t\t['pool2',[3,3],[0,0],[2,2]],\\\n",
        "\t\t['conv3',[384,256,3,3],[1,1],[1,1]],\\\n",
        "\t\t['conv4',[384,384,3,3],[1,1],[1,1]],\\\n",
        "\t\t['conv5',[256,384,3,3],[1,1],[1,1]],\\\n",
        "\t\t['pool5',[3,3],[0,0],[2,2]],\\\n",
        "\t\t['reshape'],\\\n",
        "\t\t['fc6',[4096,9216]],\\\n",
        "\t\t['fc7',[4096,4096]],\\\n",
        "\t\t['fc8',[1000,4096]]\n",
        "\t\t]\n",
        "\n",
        "tensor_size = [10,3,227,227]\n",
        "#tensor_size = [10,3,600,800]\n",
        "for layer in alexNet:\n",
        "  if 'conv' in layer[0]:\n",
        "    layer_name, kernel_size, pad_size, stride_size = layer\n",
        "    tensor_size = getSizeConv(tensor_size, kernel_size, pad_size, stride_size)\n",
        "    print(layer_name, tensor_size)\n",
        "  elif 'pool' in layer[0]:\n",
        "    layer_name, kernel_size, pad_size, stride_size = layer\n",
        "    tensor_size = getSizePool(tensor_size, kernel_size, pad_size, stride_size)\n",
        "    print(layer_name, tensor_size)\n",
        "  elif 'reshape' in layer[0]:\n",
        "    tensor_size = getSizeReshape(tensor_size)\n",
        "    print(layer[0], tensor_size)\n",
        "  elif 'fc' in layer[0]:\n",
        "    layer_name, weight_size = layer\n",
        "    tensor_size = getSizeFc(tensor_size, weight_size)\n",
        "    print(layer_name, tensor_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1 [10 96 55 55]\n",
            "pool1 [10 96 27 27]\n",
            "conv2 [ 10 256  27  27]\n",
            "pool2 [ 10 256  13  13]\n",
            "conv3 [ 10 384  13  13]\n",
            "conv4 [ 10 384  13  13]\n",
            "conv5 [ 10 256  13  13]\n",
            "pool5 [ 10 256   6   6]\n",
            "reshape [  10 9216]\n",
            "fc6 [  10 4096]\n",
            "fc7 [  10 4096]\n",
            "fc8 [  10 1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjY32gtieoVb"
      },
      "source": [
        "# <b> [13 pts] 2. Numpy/Scipy implementation</b>\n",
        "\n",
        "After knowing the I/O size of each layer, let's implement each layer in numpy/scipy packages to fully understand how things are computed in Pytorch.\n",
        "\n",
        "**Finish Problem 1 first**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjN6pTlqfQPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "6b68bd02-b191-477b-8ae6-c3ad9ae0d9f8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "# this one is slightly different from the orignal AlexNet paper\n",
        "alexnet = models.alexnet()\n",
        "alexnet.modules"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.modules of AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch.nn.modules.module.Module.modules</b><br/>def modules() -&gt; Iterator[&#x27;Module&#x27;]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py</a>Return an iterator over all modules in the network.\n",
              "\n",
              "Yields:\n",
              "    Module: a module in the network\n",
              "\n",
              "Note:\n",
              "    Duplicate modules are returned only once. In the following\n",
              "    example, ``l`` will be returned only once.\n",
              "\n",
              "Example::\n",
              "\n",
              "    &gt;&gt;&gt; l = nn.Linear(2, 2)\n",
              "    &gt;&gt;&gt; net = nn.Sequential(l, l)\n",
              "    &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\n",
              "    ...     print(idx, &#x27;-&gt;&#x27;, m)\n",
              "\n",
              "    0 -&gt; Sequential(\n",
              "      (0): Linear(in_features=2, out_features=2, bias=True)\n",
              "      (1): Linear(in_features=2, out_features=2, bias=True)\n",
              "    )\n",
              "    1 -&gt; Linear(in_features=2, out_features=2, bias=True)</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 2306);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TxwalBJetRR"
      },
      "source": [
        "## (a) [3 pts] Conv Layer\n",
        "\n",
        "In most deep learning packages, the convolution operation is \"wrongly\" defined as the correlation, i.e., dot product without flipping the kernel. In practice, it doesn't matter much; but in theory, it doesn't have the commutative property or the Fourier theorem.\n",
        "\n",
        "**Course Material: Lecture 16, page 7**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtigxR02eodG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d18b2dd-29ca-4907-8bd5-cb344c4e5114"
      },
      "source": [
        "from scipy.ndimage import correlate\n",
        "def my_conv(input_tensor, kernel_tensor, bias_tensor, pad_size, stride_size):\n",
        "  # input_tensor size: N x C_in x H x W\n",
        "  # kernel_tensor size: C_out x C_in x KH x KW\n",
        "  # stride: [Sx, Sy]\n",
        "  # pad: [Px, Py]\n",
        "  # -----\n",
        "  # output_tensor size: N x C_out x OH x OW\n",
        "  output_size = getSizeConv(input_tensor.shape, kernel_tensor.shape, pad_size, stride_size)\n",
        "  print(output_size)\n",
        "  print(bias_tensor.shape)\n",
        "  output_tensor = np.zeros(output_size)\n",
        "\n",
        "  patch_sz_h = np.array([kernel_tensor.shape[2]//2, kernel_tensor.shape[3]//2])\n",
        "  channel_h = input_tensor.shape[1]//2\n",
        "  for batch_id in range(output_size[0]):\n",
        "    for kernel_id in range(output_size[1]):\n",
        "      # 1. convolution\n",
        "      ## correlate: it only does valid convolution\n",
        "      out_channel = correlate(input_tensor[batch_id], kernel_tensor[kernel_id], mode='constant')\n",
        "      ## crop out the valid part\n",
        "      crop = patch_sz_h-pad_size\n",
        "      out_channel = out_channel[channel_h, crop[0]:-crop[0], crop[1]:-crop[1]]\n",
        "\n",
        "      ### Your code starts here\n",
        "      # 2. apply strides\n",
        "      out_channel = out_channel[::stride_size[0], ::stride_size[1]]\n",
        "\n",
        "      # 3. add bias_tensor\n",
        "      out_channel = out_channel + bias_tensor[kernel_id]\n",
        "      # 4. assign output\n",
        "      output_tensor[batch_id, kernel_id] = out_channel\n",
        "      ### Your code ends here\n",
        "\n",
        "\n",
        "  print(out_channel.shape)\n",
        "  return output_tensor\n",
        "\n",
        "test_tensor = torch.randn([2,3,224,224])\n",
        "conv1 = alexnet._modules['features'][0]\n",
        "\n",
        "outConv_my = my_conv(test_tensor.detach().numpy(), conv1.weight.detach().numpy(), conv1.bias.detach().numpy(), conv1.padding, conv1.stride)\n",
        "outConv_pt = conv1(test_tensor).detach().numpy()\n",
        "\n",
        "np.abs(outConv_pt - outConv_my).max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2 64 55 55]\n",
            "(64,)\n",
            "(55, 55)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.3113021850585938e-06"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxfz8Kibnlbc"
      },
      "source": [
        "## (b) [2 pts] ReLU Layer\n",
        "\n",
        "**Course Material: Lecture 14, page 29**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YIQA_sGfh_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a2a544-5524-4d56-e6ce-86d05bbee980"
      },
      "source": [
        "def my_relu(input_tensor):\n",
        "  output_tensor = input_tensor.copy()\n",
        "  #### Your code starts here\n",
        "  output_tensor = np.maximum(input_tensor, 0)\n",
        "\n",
        "  #### Your code ends here\n",
        "  return output_tensor\n",
        "\n",
        "test_tensor = torch.randn([2,64,55,55])\n",
        "relu1 = alexnet._modules['features'][1]\n",
        "\n",
        "outReLU_my = my_relu(test_tensor.detach().numpy())\n",
        "outReLU_pt = relu1(test_tensor).detach().numpy()\n",
        "\n",
        "np.abs(outReLU_my - outReLU_pt).max()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VLlQzQioTsj"
      },
      "source": [
        "## (c) [2 pts] Pooling Layer\n",
        "- maxpool belongs to the family of rank filter, e.g, median filter\n",
        "- avgpool is the same as the box filter\n",
        "\n",
        "**Course materials: Lecture 16, page 15-20**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWJf5_JHoT0u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbf472a5-2a53-4a41-dc4c-275402e801f1"
      },
      "source": [
        "def my_pool(pool_ops, input_tensor, kernel_size, pad_size, stride_size):\n",
        "  # input_tensor size: N x C_in x H x W\n",
        "  # kernel_size: KH x KW\n",
        "  # pad_size: [Px, Py]\n",
        "  # stride_size: [Sx, Sy]\n",
        "  # output_size: N x C_in x OH x OW\n",
        "  output_size = getSizePool(input_tensor.shape, kernel_size, pad_size, stride_size)\n",
        "  output_tensor = np.zeros(output_size)\n",
        "\n",
        "  if isinstance(kernel_size, int):\n",
        "    kernel_size = [kernel_size, kernel_size]\n",
        "  if isinstance(stride_size, int):\n",
        "    stride_size = [stride_size, stride_size]\n",
        "  if isinstance(pad_size, int):\n",
        "    pad_size = [pad_size, pad_size]\n",
        "\n",
        "  for x in range(output_size[2]):\n",
        "    for y in range(output_size[3]):\n",
        "      patch = input_tensor[:, :, x*stride_size[0]:x*stride_size[0]+kernel_size[0],\\\n",
        "              y*stride_size[1]:y*stride_size[1]+kernel_size[1]]\n",
        "      #### Your code starts here\n",
        "      if pool_ops == 'max':\n",
        "        output_tensor[:,:,x,y] = np.max(patch, axis=(2, 3))\n",
        "      elif pool_ops == 'avg':\n",
        "        output_tensor[:,:,x,y] = np.mean(patch, axis=(2, 3))\n",
        "      #### Your code ends here\n",
        "\n",
        "  return output_tensor\n",
        "\n",
        "test_tensor = torch.randn([2,64,55,55])\n",
        "pool1 = alexnet._modules['features'][2]\n",
        "\n",
        "outPool_my = my_pool('max', test_tensor.detach().numpy(), pool1.kernel_size, pool1.padding, pool1.stride)\n",
        "outPool_pt = pool1(test_tensor).detach().numpy()\n",
        "\n",
        "# it's adaptiveavgpool instead of the regular avgpool.\n",
        "# it specifies the output size instead\n",
        "# let's hack it for now...\n",
        "# input 18x18\n",
        "# adapativeavgpool has the fixed output size [6,6]\n",
        "# ours: equivalently, kernel size [3,3] and stride size [3,3]\n",
        "pool2 = alexnet._modules['avgpool']\n",
        "test_tensor = torch.randn([2,64,18,18])\n",
        "outPool_my2 = my_pool('avg', test_tensor.detach().numpy(), 3, 0, 3)\n",
        "outPool_pt2 = pool2(test_tensor).detach().numpy()\n",
        "\n",
        "print('Max abs diff for maxpool', np.abs(outPool_my - outPool_pt).max())\n",
        "print('Max abs diff for avgpool',np.abs(outPool_my2 - outPool_pt2).max())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max abs diff for maxpool 0.0\n",
            "Max abs diff for avgpool 1.7881393432617188e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y45l4wFCwMnN"
      },
      "source": [
        "## (d) [2 pts] Dropout Layer\n",
        "\n",
        "**Course materials: Lecture 15, page 10-11**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwGxJXdquG2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c237ae4-1a3c-422d-9344-cfe6f2f9c17d"
      },
      "source": [
        "def my_drop(input_tensor, p):\n",
        "  output_tensor = input_tensor.copy()\n",
        "  #### Your code starts here\n",
        "  mask = np.random.binomial(1, 1 - p, size=input_tensor.shape)\n",
        "  output_tensor = output_tensor / (1 - p)\n",
        "  output_tensor = output_tensor * mask\n",
        "  #### Your code ends here\n",
        "  return output_tensor\n",
        "\n",
        "test_tensor = torch.randn([100,4096])\n",
        "drop1 = alexnet._modules['classifier'][0]\n",
        "\n",
        "outDrop_my = my_drop(test_tensor.detach().numpy(), drop1.p)\n",
        "outDrop_pt = drop1(test_tensor).detach().numpy()\n",
        "\n",
        "num = float(test_tensor.nelement())\n",
        "# doesn't have to be 0\n",
        "np.abs((outDrop_my==0).sum()/num - (outDrop_pt==0).sum()/num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002326660156250049"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqfML4FIr7RT"
      },
      "source": [
        "## (e) [2 pts] Fully-connected Layer\n",
        "\n",
        "**Course Material: Lecture 14, page 23**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "039F6lFJfmNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28abc2d-af87-4998-e00b-32752c4790a1"
      },
      "source": [
        "def my_fc(input_tensor, weight_tensor, bias_tensor):\n",
        "  # N: batch size/number of samples\n",
        "  # input_tensor size: N x L\n",
        "  # weight_size: M x L\n",
        "  # output_size: N x M\n",
        "\n",
        "  output_size = getSizeFc(input_tensor.shape, weight_tensor.shape)\n",
        "  output_tensor = np.zeros(output_size)\n",
        "\n",
        "  #### Your code starts here\n",
        "  for sample_id in range(input_tensor.shape[0]):\n",
        "    # hint: numpy matrix multiplication is np.matmul\n",
        "    # a*b is the element-wise multiplication\n",
        "    output_tensor[sample_id] = np.matmul(input_tensor[sample_id], weight_tensor.T) + bias_tensor\n",
        "  #### Your code ends here\n",
        "\n",
        "  return output_tensor\n",
        "\n",
        "test_tensor = torch.randn([2,4096])\n",
        "fc7 = alexnet._modules['classifier'][4]\n",
        "\n",
        "outFc_my = my_fc(test_tensor.detach().numpy(), fc7.weight.detach().numpy(), fc7.bias.detach().numpy())\n",
        "outFc_pt = fc7(test_tensor).detach().numpy()\n",
        "\n",
        "np.abs(outFc_my - outFc_pt).max()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2218952178955078e-06"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zwAaGtUzLLI"
      },
      "source": [
        "## (f) [2 pts] Softmax Layer\n",
        "\n",
        "**Course material: Lecture 14, page 17**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyNaNxvtzLT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11cace36-66c6-471a-89a9-3f7b0a2ed50a"
      },
      "source": [
        "def my_softmax(input_tensor):\n",
        "  output_tensor = input_tensor.copy()\n",
        "\n",
        "  #### Your code starts here\n",
        "  max_val = np.max(output_tensor, axis=-1, keepdims=True)\n",
        "  output_tensor = output_tensor - max_val\n",
        "  exp_vals = np.exp(output_tensor)\n",
        "  normalization_factor = np.sum(exp_vals, axis=-1, keepdims=True)\n",
        "  output_tensor = exp_vals / normalization_factor\n",
        "\n",
        "  #### Your code ends here\n",
        "  return output_tensor\n",
        "\n",
        "test_tensor = torch.randn([2,4096])\n",
        "\n",
        "outSoftmax_my = my_softmax(test_tensor.detach().numpy())\n",
        "outSoftmax_pt = F.softmax(test_tensor).detach().numpy()\n",
        "\n",
        "np.abs(outSoftmax_my - outSoftmax_pt).max()\n",
        "print('Max abs diff for softmax', np.abs(outSoftmax_my - outSoftmax_pt).max())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max abs diff for softmax 1.8626451e-09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-6e20cc37d201>:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  outSoftmax_pt = F.softmax(test_tensor).detach().numpy()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wyM76UqQ1yR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}